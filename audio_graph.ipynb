{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre recognition: graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio genre recognition pipeline:\n",
    "1. GTZAN\n",
    "1. pre-processing\n",
    "1. graph construction\n",
    "1. unsupervised feature extraction\n",
    "1. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constructs a KNN graph from samples and compute the normalized graph Laplacian for future use as a regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters:\n",
    "* K: number of nearest neighbors (minimum number of edges per vertex).\n",
    "* dm: distance metric (euclidean or cosine).\n",
    "* Csigma: constant which multiplies the mean of the weights when computing the $\\sigma$ of the Gaussian kernel.\n",
    "* diag: wether we want the diagonal of the weight matrix to be zero (no self-connected vertices) or ones (may help to regularize the normalized Laplacian, no difference for the un-normalized one).\n",
    "* laplacian: normalized or un-normalized Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10 + 1  # 5 to 10 + 1 for self-reference\n",
    "dm = 'cosine'  # euclidean or cosine\n",
    "Csigma = 1\n",
    "diag = True\n",
    "laplacian = 'normalized'  # normalized or unnormalized\n",
    "\n",
    "tol = 1e-5  # Tolerance when asserting values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pyflann\n",
    "#import sklearn.neighbors\n",
    "#from annoy import AnnoyIndex\n",
    "import scipy.sparse\n",
    "\n",
    "toverall = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'audio.hdf5')\n",
    "with h5py.File(filename, 'r') as audio:\n",
    "    X = audio.get('Xs')\n",
    "    Ngenres, Nclips, Nframes, _, n = X.shape\n",
    "    #Ngenres, Nclips, Nframes = 2, 10, 644  # Reduced dataset.\n",
    "    X = X[:Ngenres,:Nclips,:Nframes,...]  # Load into memory.\n",
    "X.resize(Ngenres * Nclips * Nframes * 2, n)\n",
    "print('Data: {}, {}'.format(X.shape, X.dtype))\n",
    "\n",
    "# To be done in pre-processing.\n",
    "X -= np.min(X, axis=0)\n",
    "X /= np.max(X, axis=0)\n",
    "\n",
    "# Normalize: put each sample on the unit sphere.\n",
    "if dm is 'cosine':\n",
    "    #print(np.sum(np.sqrt(np.sum(X**2, axis=1)) == 0))\n",
    "    X += 1e-20  # To avoid division by zero if we have a null vector.\n",
    "    X /= np.sqrt(np.sum(X**2, axis=1))[:,np.newaxis]\n",
    "    assert np.linalg.norm(X[0,:]) - 1 < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Several libraries for KNN. FLANN is the fastest.\n",
    "* We can obtain greater accuracy (when using approximate methods) by asking for $10K$ neighbors, then sort and keep the $K$ closest ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms: brute force, kd-tree, ball tree.\n",
    "* Much slower than FLANN.\n",
    "* Takes 3.23s for 4000 samples with *ball_tree*.\n",
    "* Takes 3.03s for 4000 samples with *kd_tree*.\n",
    "* Takes 0.40s for 4000 samples with *brute*.\n",
    "* From doc: not likely to perform well in high dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    params = {'n_neighbors': K}\n",
    "    params['algorithm'] = 'brute'  # ball_tree, kd_tree, brute\n",
    "    params['metric'] = 'euclidean'  # minkowski, euclidean, cosine\n",
    "    nbrs = sklearn.neighbors.NearestNeighbors(**params).fit(X)\n",
    "\n",
    "    tstart = time.time()\n",
    "    dist, idx = nbrs.kneighbors(X)\n",
    "    print('Elapsed time: {:.2f} seconds'.format(time.time() - tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn approximate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm: forest of locality sensitive hashes (LSH).\n",
    "* Return the cosine distance.\n",
    "* Takes 15s for 4000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    tstart = time.time()\n",
    "    lshf = sklearn.neighbors.LSHForest()\n",
    "    lshf.fit(X)\n",
    "    print('Elapsed time: {:.2f} seconds'.format(time.time() - tstart))\n",
    "\n",
    "    tstart = time.time()\n",
    "    dist, idx = lshf.kneighbors(X, n_neighbors=K)\n",
    "    print('Elapsed time: {:.2f} seconds'.format(time.time() - tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms: brute force, randomized kd-tree, hierarchical k-means.\n",
    "* Well parallelized with OpenMP.\n",
    "* Linear search is brute force, much slower. But gives perfect NN.\n",
    "* Returned distances are squared Euclidean distances.\n",
    "* The tradeoff between speed and accuracy (in the autotuned setting) is set via *target_precision*.\n",
    "\n",
    "Time efficiency:\n",
    "* Default algorithm (which probably construct some index) takes 120s for the entire dataset. But it probably makes large approximations.\n",
    "* With target_precision=.9 (autotuned):\n",
    "    * 100s for 40'000 samples (dim=96)\n",
    "    * 620s for 1'288'000 samples (dim=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    flann = pyflann.FLANN()\n",
    "    flann.build_index(X)  # autotuned\n",
    "    idx, dist = flann.nn_index(X, K)\n",
    "    flann.delete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    tstart = time.time()\n",
    "    #idx, dist = flann.nn(X, X, K, algorithm='linear')\n",
    "    idx, dist = pyflann.FLANN().nn(X, X, K,\n",
    "                                         algorithm='autotuned',\n",
    "                                         target_precision=.99)\n",
    "    #idx, dist = flann.nn(X, X, K)\n",
    "    print('Elapsed time: {:.2f} seconds'.format(time.time() - tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm: LSH via random projections.\n",
    "* From Spotify.\n",
    "* Can only add and query one item at a time.\n",
    "* Crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    a = AnnoyIndex(X.shape[1], metric='angular')  # euclidean, angular\n",
    "    for i in range(X.shape[0]):\n",
    "        a.add_item(i, X[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm: locality sensitive hashes (LSH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot exclude self-references (because the testset is the dataset) here as we have no guarantee that the first column points to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert idx.shape == (X.shape[0], K)\n",
    "assert idx.shape == dist.shape\n",
    "print('All self-referenced in the first column: {}'.format(np.alltrue(dist[:,0] == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distance:\n",
    "* Euclidean: $d_{i,j} = \\|x_i - x_j\\|_2$.\n",
    "* Cosine: $d_{i,j} = 1 - cos(\\theta) = 1 - <x_i, x_j> = \\frac{1}{2} \\|x_i - x_j\\|_2^2$ if the space is positive and all $x_i$ are normalized (i.e. the samples lie on the unit sphere). The cosine similarity measure is defined by $cos(\\theta) = \\frac{<x_i, x_j>}{\\|x_i\\|_2 \\|x_j\\|_2}$. Demonstration: $\\|x_i - x_j\\|_2^2 = <x_i - x_j, x_i - x_j> = <x_i, x_i> + <x_j, x_j> - 2 <x_i, x_j>$. If all $x_i$ are normalized then $<x_i, x_i> = <x_j, x_j> = 1$ thus $\\|x_i - x_j\\|_2^2 = 2 - 2 <x_i, x_j>$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dm is 'euclidean':\n",
    "    # We could even omit the square root.\n",
    "    dist = np.sqrt(dist)\n",
    "elif dm is 'cosine':\n",
    "    # Here the division.\n",
    "    dist = dist / 2\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "print('dist in [{}, {}]'.format(dist.min(), dist.max()))\n",
    "\n",
    "# Verification.\n",
    "i, k = 14, 3\n",
    "j = idx[i, k]\n",
    "if dm is 'euclidean':\n",
    "    d = np.linalg.norm(X[i,:] - X[j,:])\n",
    "elif dm is 'cosine':\n",
    "    assert np.linalg.norm(X[i,:]) - 1 < tol\n",
    "    assert np.linalg.norm(X[j,:]) - 1 < tol\n",
    "    d = 1 - np.sum(X[i,:] * X[j,:])\n",
    "assert abs(dist[i,k] - d) < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge weights are defined by a Gaussian kernel. The scale is defined according to [Perona'04]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma = Csigma * np.mean(dist[:,-1])\n",
    "i = 73; assert dist[i,:].max() == dist[i,-1]\n",
    "w = np.exp(-dist / sigma)\n",
    "print('w in [{}, {}]'.format(w.min(), w.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate indices via an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class indices(object):\n",
    "    def __init__(self, N, K):\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.n = 0\n",
    "        self.k = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N * self.K\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3.\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    # Python 2.\n",
    "    def next(self):\n",
    "        self.k += 1\n",
    "        if self.k > self.K:\n",
    "            self.k = 1\n",
    "            self.n += 1\n",
    "            if self.n >= self.N:\n",
    "                self.k = 0\n",
    "                self.n = 0\n",
    "                raise StopIteration()\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the sparse weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = list(indices(X.shape[0], K))\n",
    "j = idx.flat  # flat, ravel(), flatten()\n",
    "v = w.flat\n",
    "Nvertices = X.shape[0]\n",
    "\n",
    "# COO is good for matrix construction (LIL to insert elements).\n",
    "W = scipy.sparse.coo_matrix((v, (i,j))).tolil()\n",
    "del i, j, v\n",
    "assert W.shape == (Nvertices, Nvertices)\n",
    "\n",
    "# It should be True... False means that KNN did not find\n",
    "# two identical vectors to be close enough (approximation).\n",
    "Nones = np.sum(W.diagonal() == 1)\n",
    "print('Ones on the diagonal: {} (over {})'.format(Nones, Nvertices))\n",
    "print('assert: {}'.format(Nones == Nvertices))\n",
    "\n",
    "if diag:\n",
    "    W.setdiag(Nvertices*[1])\n",
    "else:\n",
    "    W.setdiag(Nvertices*[0])\n",
    "assert np.all(W.diagonal() == diag)\n",
    "\n",
    "# CSR is good for arithmetic operations.\n",
    "W = W.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want an undirected graph, i.e. a symmetric weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#W = W/2 + W.T/2\n",
    "\n",
    "#W = np.maximum(W, W.T)  # Does not work for sparse matrices.\n",
    "bigger = W.T > W\n",
    "W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "del bigger\n",
    "assert (W - W.T).sum() < tol  # Should be symmetric.\n",
    "\n",
    "if diag:\n",
    "    assert np.all(W.diagonal() == 1)\n",
    "else:\n",
    "    assert np.all(W.diagonal() == 0)\n",
    "print('W in [{}, {}]'.format(W.min(), W.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could verify that the matrix is positive-semidefinite by computing its Cholesky decomposition (CHOLMOD for sparse matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the degree matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = W.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Laplacian or the symmetric normalized Laplacian matrix (which needs $D^{-1/2}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if laplacian is 'unnormalized':\n",
    "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "    L = D - W\n",
    "elif laplacian is 'normalized':\n",
    "    d = 1 / np.sqrt(d)\n",
    "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "    I = scipy.sparse.identity(Nvertices)\n",
    "    L = I - D * W * D\n",
    "    del I\n",
    "else:\n",
    "    raise ValueError\n",
    "del d, D\n",
    "\n",
    "assert (L - L.T).sum() < tol  # Should be symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways of saving sparse matrices with HDF5:\n",
    "* Store the underlying dense matrices who support the sparse representation.\n",
    "* Store as a dense matrix, leveraging HDF5 compression. Memory is still needed to convert the sparse matrix to a dense one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'graph.hdf5')\n",
    "\n",
    "# Remove existing HDF5 file without warning if non-existent.\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with h5py.File(filename, 'w') as graph:\n",
    "\n",
    "    # Metadata: hyper-parameters.\n",
    "    for attr in ('K', 'dm', 'Csigma', 'diag', 'laplacian'):\n",
    "        graph.attrs[attr] = globals()[attr]\n",
    " \n",
    "    # Data: weight and Laplacian matrices.\n",
    "    for mat in ('W', 'L'):\n",
    "        m = globals()[mat]\n",
    "        for par in ('data', 'indices', 'indptr', 'shape'):\n",
    "            arr = np.array(getattr(m, par))\n",
    "            graph.create_dataset(mat+'_'+par, data=arr)\n",
    "\n",
    "    # Show datasets, their dimensionality and data type.\n",
    "    print('Datasets:')\n",
    "    for dname, dset in graph.items():\n",
    "        print('  {:10}: {:10}, {}'.format(dname, dset.shape, dset.dtype))\n",
    "\n",
    "    # Display HDF5 attributes.\n",
    "    print('Attributes:')\n",
    "    for name, value in graph.attrs.items():\n",
    "        print('  {} = {}'.format(name, value))\n",
    "\n",
    "print('Overall time: {:.2f} seconds'.format(time.time() - toverall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
