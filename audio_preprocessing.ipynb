{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy auto-encoder: audio pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice clips in frames, apply a constant-Q transform (CQT) then some local contrast normalization (LCN). Processed audio and labels are stored in HDF5 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import h5py\n",
    "\n",
    "print('Software versions:')\n",
    "for pkg in [np, librosa]:\n",
    "    print('  {}: {}'.format(pkg.__name__, pkg.__version__))\n",
    "\n",
    "# Much faster computation of the CQT if available.\n",
    "# Provided by scikits.samplerate through libsamplerate (SRC).\n",
    "print('\\nlibrosa HAS_SAMPLERATE: {}'.format(librosa.core._HAS_SAMPLERATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio data from the GTZAN dataset has been previously stored in the HDF5 format which allows us to read and write data without the need to load the whole data set into memory via memory mapping ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'gtzan.hdf5')\n",
    "gtzan = h5py.File(filename, 'r')\n",
    "\n",
    "# Display HDF5 attributes.\n",
    "print('Attributes:')\n",
    "for name, value in gtzan.attrs.items():\n",
    "    print('  {} = {}'.format(name, value))\n",
    "\n",
    "# List the stored datasets.\n",
    "print('Datasets: {}'.format(', '.join(gtzan)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each clip is divided into short frames of $n_a = 1024$ samples.\n",
    "* There is a 50% overlap between consecutive frames which adds redundancy in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "na = 1024\n",
    "# Aligned.\n",
    "N1 = int(np.floor(float(gtzan.attrs['Nsamples']) / na))\n",
    "# Overlap (redundant).\n",
    "N2 = int(np.floor(float(gtzan.attrs['Nsamples']) / na - 0.5))\n",
    "Nframes = min(N1, N2)\n",
    "Nclips = gtzan.attrs['Nclips']\n",
    "Ngenres = gtzan.attrs['Ngenres']\n",
    "del(N1, N2)\n",
    "print('N = {:,} (Nframes={}) frames of na = {} samples'.format(\n",
    "        Ngenres * Nclips * Nframes * 2, Nframes, na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant-Q transform (CQT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CQT as a dimensionality reduction (from $n_a=1024$ to $n_s=96$) and a feature extraction tool:\n",
    "* Span $N_o = 4$ octaves from $C_2$ to $C_6$ where $C_4$ is the middle $C$ in the scientific pitch notation.\n",
    "* Western music uses 12TET (twelve-tone equal temperament): 7 notes and 12 semitones per octave. We use 24 bins per octave to achieve a quarter-tone resolution.\n",
    "* It gives us $n_s = 96$ filters, the dimensionality of the input data to the auto-encoder.\n",
    "\n",
    "Open questions:\n",
    "* Should we truncate the clip to a mutliple of $n_a$ ?\n",
    "* How to handle boundary conditions ? Discard ? Keep ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CQT filters.\n",
    "ns = 96\n",
    "No = 4\n",
    "print('ns = {} filters spanning No = {} octaves'.format(ns, No))\n",
    "print('  --> resolution of {} bins per octave'.format(ns/No))\n",
    "\n",
    "# This MIDI implementation assigns middle C (note 60) to C5 not C4 !\n",
    "# It may also be C3, there is no standardisation.\n",
    "# It is not consistent with the scientific pitch notation.\n",
    "assert librosa.note_to_midi('C5') == 60\n",
    "# Tuning standard A4 = 440 Hz (A440) becomes A5 = 440Hz.\n",
    "assert librosa.midi_to_hz(librosa.note_to_midi('A5')) == 440\n",
    "assert librosa.midi_to_hz(69) == 440\n",
    "\n",
    "# We should thus use C3 and C7 instead of C2 and C6...\n",
    "nmin, nmax = 'C3', 'C7'\n",
    "fmin = librosa.midi_to_hz(librosa.note_to_midi(nmin))\n",
    "fmax = librosa.midi_to_hz(librosa.note_to_midi(nmax))\n",
    "assert fmax / fmin == 2**No  # By definition of an octave.\n",
    "print('fmin = {:.2f} Hz ({}), fmax = {:.2f} Hz ({})'.format(\n",
    "        fmin[0], nmin, fmax[0], nmax))\n",
    "\n",
    "# librosa CQT parameters.\n",
    "rosaparams = {'sr':gtzan.attrs['sr'], 'hop_length':na, 'fmin':fmin,\n",
    "          'n_bins':ns, 'bins_per_octave':ns/No}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five dimensions:\n",
    "1. Genre number in [0,Ngenres-1]. The *genres* attribute can be indexed with this number to retrieve the name of the genre.\n",
    "2. Clip number in [0,Nclips-1].\n",
    "3. Frame number in [0,Nframes-1].\n",
    "4. Overlap in [0,1]: 0 for the aligned frames, 1 for the overlapped ones.\n",
    "5. Data dimensionality in [0:n].\n",
    "\n",
    "Three 5-dimensional HDF5 datasets:\n",
    "3. *Xa*: raw audio of the frame, dimensionality $n=n_a$\n",
    "4. *Xs*: CQT spectrogram, dimensionality $n=n_s$\n",
    "5. *Xn*: LCN normalized spectrogram, dimensionality $n=n_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'audio.hdf5')\n",
    "\n",
    "# Remove existing HDF5 file without warning if non-existent.\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# Create HDF5 file and datasets.\n",
    "audio = h5py.File(filename, 'w')\n",
    "\n",
    "# Metadata.\n",
    "audio.attrs['sr'] = gtzan.attrs['sr']\n",
    "genres = gtzan.keys()\n",
    "dtype = 'S{}'.format(max([len(genre) for genre in genres]))\n",
    "audio.attrs['labels'] = np.array(genres, dtype=dtype)\n",
    "\n",
    "# Data.\n",
    "Xa = audio.create_dataset('Xa', (Ngenres, Nclips, Nframes, 2, na), dtype='float32')\n",
    "Xs = audio.create_dataset('Xs', (Ngenres, Nclips, Nframes, 2, ns), dtype='float32')\n",
    "#Xn = f.create_dataset('Xn', (ns, N), dtype='float32')\n",
    "\n",
    "# Show datasets, their dimensionality and data type.\n",
    "print('Datasets:')\n",
    "for dname, dset in audio.items():\n",
    "    print('  {:2}: {:16}, {}'.format(dname, dset.shape, dset.dtype))\n",
    "\n",
    "# Display HDF5 attributes.\n",
    "print('Attributes:')\n",
    "for name, value in audio.attrs.items():\n",
    "    print('  {} = {}'.format(name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill $X_s \\in R^{n_s \\times N}$ with the CQT spectrogram of all $N$ frames. Store the raw audio in $X_a \\in R^{n_a \\times N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing function load audio data, compute the CQT and store the result in HDF5 datasets along with raw audio and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'newshape':(Nframes, na), 'order':'C'}\n",
    "\n",
    "def process(genre, clip):\n",
    "    \"\"\"Usage: process(1, 2)\"\"\"\n",
    "\n",
    "    # Load audio.\n",
    "    y1 = gtzan[genres[genre]][:,clip]  # Aligned frames.\n",
    "    y2 = y1[na/2:]  # Overlaped frames.\n",
    "\n",
    "    # Store raw audio.\n",
    "    Xa[genre,clip,:,0,:] = np.reshape(y1[:na*Nframes], **params)\n",
    "    Xa[genre,clip,:,1,:] = np.reshape(y2[:na*Nframes], **params)\n",
    "\n",
    "    # Ensure that the signal is correctly reshaped.\n",
    "    i = int(np.floor(Nframes * np.random.uniform()))\n",
    "    assert np.alltrue(Xa[genre,clip,i,0,:] == y1[i*na:i*na+na])\n",
    "    assert np.alltrue(Xa[genre,clip,i,1,:] == y1[na/2+i*na:na/2+i*na+na])\n",
    "\n",
    "    # Store spectrogram. Drop the last one which consists mostly\n",
    "    # of padded data (and keep the same size as Xa).\n",
    "    Xs[genre,clip,:,0,:] = librosa.cqt(y1, **rosaparams)[:,:-1].T\n",
    "    Xs[genre,clip,:,1,:] = librosa.cqt(y2, **rosaparams)[:,:-1].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process a single clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#process(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the entire GTZAN dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ngenres, Nclips = 2, 10\n",
    "tstart = time.time()\n",
    "for genre in range(Ngenres):\n",
    "    for clip in range(Nclips):\n",
    "        process(genre, clip)\n",
    "print('Elapsed time: {:.0f} seconds'.format(time.time() - tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Close HDF5 data stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtzan.close()\n",
    "audio.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
