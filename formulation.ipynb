{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy auto-encoder: algorithm formulation and synthetic tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from pyunlocbox import functions, solvers\n",
    "import matplotlib, pyunlocbox  # For versions only.\n",
    "print('Software versions:')\n",
    "for pkg in [np, matplotlib, pyunlocbox]:\n",
    "    print('  %s: %s' % (pkg.__name__, pkg.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear least square fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X \\in R^{n \\times N}$ and $D \\in R^{n \\times m}$, solve $\\min\\limits_{Z \\in R^{m \\times N}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data and dictionary.\n",
    "X = np.array([[2,-5,0],[-9,3,1],[5,7,-6]]).transpose()\n",
    "D = np.array([[1,0,1],[-1,2,4]]).transpose()  # Overdetermined (no solution).\n",
    "D = np.array([[1,0,0],[0,0,0.5],[0,2,0]]).transpose()  # Determined (a unique solution).\n",
    "D = np.array([[1,0,1],[0,-4,0.5],[7,-2,0],[-1,2,4]]).transpose()  # Indeterminate (infinite number of solutions).\n",
    "print('X = \\n%s' % (X,))\n",
    "print('D = \\n%s' % (D,))\n",
    "(n, N) = np.shape(X)\n",
    "(n, m) = np.shape(D)\n",
    "\n",
    "# Model hyper-parameters.\n",
    "l_d = 1\n",
    "\n",
    "# Algorithm numeric parameters.\n",
    "rtol = 1e-4\n",
    "\n",
    "# Loss function.\n",
    "f_z = functions.norm_l2(lambda_=l_d/2., A=D, y=X, tight=False)\n",
    "\n",
    "# Convex minimization (step size should be smaller than 1/L).\n",
    "L = l_d * la.norm(np.dot(D.T, D))  # Lipschitz continuous gradient.\n",
    "solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "Z = np.random.normal(size=(m, N))\n",
    "ret = solvers.solve([f_z], Z, solver, rtol=rtol)\n",
    "\n",
    "# Display solution.\n",
    "Z = ret['sol']\n",
    "print('Z = \\n%s' % (Z,))\n",
    "print('DZ = \\n%s' % (np.dot(D, Z),))\n",
    "\n",
    "# Visual convergence analysis.\n",
    "objective = np.array(ret['objective'])\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective[:,0], label='objective')\n",
    "plt.title('Convergence')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X \\in R^{n \\times N}$ and $D \\in R^{n \\times m}$, solve $\\min\\limits_{Z \\in R^{m \\times N}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2 + \\|Z\\|_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data and dictionary.\n",
    "X = np.array([[2,-5,0],[-9,3,1],[5,7,-6]]).transpose()\n",
    "D = np.array([[1,0,1],[-1,2,4]]).transpose()  # Under-complete.\n",
    "D = np.array([[1,0,0],[0,0,0.5],[0,2,0]]).transpose()  # Complete.\n",
    "D = np.array([[1,0,1],[0,-4,0.5],[7,-2,0],[-1,2,4]]).transpose()  # Over-complete.\n",
    "print('X = \\n%s' % (X,))\n",
    "print('D = \\n%s' % (D,))\n",
    "(n, N) = np.shape(X)\n",
    "(n, m) = np.shape(D)\n",
    "\n",
    "# Model hyper-parameters.\n",
    "l_d = 1\n",
    "\n",
    "# Algorithm numeric parameters.\n",
    "rtol = 1e-4\n",
    "\n",
    "# Loss functions.\n",
    "f_z = functions.norm_l2(lambda_=l_d/2., A=D, y=X, tight=False)\n",
    "g_z = functions.norm_l1()\n",
    "\n",
    "# Convex minimization (step size should be smaller than 1/L).\n",
    "L = l_d * la.norm(np.dot(D.T, D))  # Lipschitz continuous gradient.\n",
    "solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "Z = np.random.normal(size=(m, N))\n",
    "ret = solvers.solve([f_z, g_z], Z, solver, rtol=rtol)\n",
    "\n",
    "# Display solution.\n",
    "Z = ret['sol']\n",
    "print('Z = \\n%s' % (Z,))\n",
    "print('DZ = \\n%s' % (np.dot(D, Z),))\n",
    "\n",
    "# Visual convergence analysis.\n",
    "objective = np.array(ret['objective'])\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective[:, 0], label='data term')\n",
    "plt.semilogy(objective[:, 1], label='prior term')\n",
    "plt.semilogy(np.sum(objective, axis=1), label='global objective')\n",
    "plt.title('Convergence')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X \\in R^{n \\times N}$, solve $\\min\\limits_{Z \\in R^{m \\times N}, D \\in R^{n \\times m}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2 + \\|Z\\|_1$ s.t. $\\|d_i\\|_2 \\leq 1, i = 1, \\ldots, m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data.\n",
    "X = np.array([[2,-5,0],[-9,3,1],[5,7,-6],[-1,3,0],[9,4,7]]).transpose()\n",
    "print('X = \\n%s' % (X,))\n",
    "(n, N) = np.shape(X)\n",
    "\n",
    "# Model hyper-parameters.\n",
    "l_d = 5\n",
    "m = 4\n",
    "\n",
    "# Solver numeric parameters.\n",
    "N_outer = 30\n",
    "rtol = 1e-4\n",
    "\n",
    "# Static loss function definitions.\n",
    "g_z = functions.norm_l1()\n",
    "g_d = functions.proj_b2(epsilon=1)  # L2-ball indicator function.\n",
    "\n",
    "# Initialization.\n",
    "Z = np.random.normal(size=(m, N))\n",
    "D = np.random.normal(size=(n, m))\n",
    "objective_z = []\n",
    "objective_d = []\n",
    "objective_g = []\n",
    "\n",
    "# Multi-variate non-convex optimization (outer loop).\n",
    "for n in np.arange(N_outer):\n",
    "    \n",
    "    # Convex minimization for Z.\n",
    "    f_z = functions.norm_l2(lambda_=l_d/2., A=D, y=X, tight=False)\n",
    "    L = l_d * la.norm(np.dot(D.T, D))  # Lipschitz continuous gradient.\n",
    "    solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "    ret = solvers.solve([f_z, g_z], Z, solver, rtol=rtol, verbosity='NONE')\n",
    "    Z = ret['sol']\n",
    "    objective_z.extend(ret['objective'])\n",
    "    objective_d.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    \n",
    "    # Convex minimization for D.\n",
    "    f_d = functions.norm_l2(lambda_=l_d/2., A=Z.T, y=X.T, tight=False)\n",
    "    L = l_d * la.norm(np.dot(Z, Z.T))  # Lipschitz continuous gradient.\n",
    "    solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "    ret = solvers.solve([f_d, g_d], D.T, solver, rtol=rtol, verbosity='NONE')\n",
    "    D = ret['sol'].T\n",
    "    objective_d.extend(ret['objective'])\n",
    "    objective_z.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    \n",
    "    # Global objective (as an indicator, g_d is 0).\n",
    "    objective_g.append(f_z.eval(Z) + g_z.eval(Z) + f_d.eval(D.T))\n",
    "\n",
    "# Display solution.\n",
    "print('Z = \\n%s' % (Z,))\n",
    "print('D = \\n%s' % (D,))\n",
    "print('|d| = %s' % ([la.norm(D[:,k]) for k in np.arange(np.size(D,1))],))\n",
    "print('DZ = \\n%s' % (np.dot(D, Z),))\n",
    "\n",
    "# Visual convergence analysis.\n",
    "print('f_z(Z) + g_z(Z) + f_d(D) = %f' % (objective_g[-1],))\n",
    "objective = np.concatenate((np.array(objective_z), np.array(objective_d)), axis=1)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective[:, 0], label='Z: data term')\n",
    "plt.semilogy(objective[:, 1], label='Z: prior term')\n",
    "#plt.semilogy(np.sum(objective[:,0:2], axis=1), label='Z: global')\n",
    "plt.semilogy(objective[:, 2], label='D: data term')\n",
    "plt.title('Sub-problems convergence')\n",
    "plt.xlabel('Iteration number (inner loops)')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.legend(); plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective_g)\n",
    "plt.title('Global convergence')\n",
    "plt.xlabel('Iteration number (outer loop)')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X \\in R^{n \\times N}$, solve $\\min\\limits_{Z \\in R^{m \\times N}, D \\in R^{n \\times m}, E \\in R^{m \\times n}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2 + \\frac{\\lambda_e}{2} \\|Z - EX\\|_F^2 + \\|Z\\|_1$ s.t. $\\|d_i\\|_2 \\leq 1$, $\\|e_k\\|_2 \\leq 1$, $i = 1, \\ldots, m$, $k = 1, \\ldots, n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data.\n",
    "X = np.array([[2,-5,0],[-9,3,1],[5,7,-6],[-1,3,0],[9,4,7]]).transpose()\n",
    "print('X = \\n%s' % (X,))\n",
    "(n, N) = np.shape(X)\n",
    "\n",
    "# Model hyper-parameters.\n",
    "l_d = 5\n",
    "l_e = 5\n",
    "m = 3\n",
    "\n",
    "# Solver numeric parameters.\n",
    "N_outer = 20\n",
    "rtol = 1e-4\n",
    "\n",
    "# Static loss function definitions.\n",
    "g_z = functions.norm_l1()\n",
    "g_de = functions.proj_b2(epsilon=1)  # L2-ball indicator function.\n",
    "\n",
    "# Initialization.\n",
    "Z = np.random.normal(size=(m, N))\n",
    "D = np.random.normal(size=(n, m))\n",
    "E = np.random.normal(size=(m, n))\n",
    "objective_z = []\n",
    "objective_d = []\n",
    "objective_e = []\n",
    "objective_g = []\n",
    "\n",
    "# Multi-variate non-convex optimization (outer loop).\n",
    "for n in np.arange(N_outer):\n",
    "    \n",
    "    # Convex minimization for Z.\n",
    "    f_zd = functions.norm_l2(lambda_=l_d/2., A=D, y=X, tight=False)\n",
    "    f_ze = functions.norm_l2(lambda_=l_e/2., y=np.dot(E,X))\n",
    "    f_z = functions.func()\n",
    "    f_z._eval = lambda Z: f_zd.eval(Z) + f_ze.eval(Z)\n",
    "    f_z._grad = lambda Z: f_zd.grad(Z) + f_ze.grad(Z)\n",
    "    L = l_e + l_d * la.norm(np.dot(D.T, D))  # Lipschitz continuous gradient.\n",
    "    solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "    ret = solvers.solve([f_z, g_z], Z, solver, rtol=rtol, verbosity='NONE')\n",
    "    Z = ret['sol']\n",
    "    objective_z.extend(ret['objective'])\n",
    "    objective_d.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    objective_e.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    \n",
    "    # Convex minimization for D.\n",
    "    f_d = functions.norm_l2(lambda_=l_d/2., A=Z.T, y=X.T, tight=False)\n",
    "    L = l_d * la.norm(np.dot(Z, Z.T))  # Lipschitz continuous gradient.\n",
    "    solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "    ret = solvers.solve([f_d, g_de], D.T, solver, rtol=rtol, verbosity='NONE')\n",
    "    D = ret['sol'].T\n",
    "    objective_d.extend(ret['objective'])\n",
    "    objective_z.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    objective_e.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    \n",
    "    #E = D.T\n",
    "    \n",
    "    # Convex minimization for E.\n",
    "    f_e = functions.norm_l2(lambda_=l_e/2., A=X.T, y=Z.T, tight=False)\n",
    "    L = l_e * la.norm(np.dot(X, X.T))  # Lipschitz continuous gradient.\n",
    "    solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "    ret = solvers.solve([f_e, g_de], E.T, solver, rtol=rtol, verbosity='NONE')\n",
    "    E = ret['sol'].T\n",
    "    objective_e.extend(ret['objective'])\n",
    "    objective_z.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    objective_d.extend(np.zeros(np.shape(ret['objective'])))\n",
    "    \n",
    "    #D = E.T\n",
    "    \n",
    "    # Global objective (as indicators, g_d and g_e are 0).\n",
    "    objective_g.append(f_z.eval(Z) + g_z.eval(Z) + f_d.eval(D.T) + f_e.eval(E.T))\n",
    "\n",
    "# Display solution.\n",
    "print('Z = \\n%s' % (Z,))\n",
    "print('D = \\n%s' % (D,))\n",
    "print('|d| = %s' % ([la.norm(D[:,k]) for k in np.arange(np.size(D,1))],))\n",
    "print('E = \\n%s' % (E,))\n",
    "print('|e| = %s' % ([la.norm(E[:,k]) for k in np.arange(np.size(E,1))],))\n",
    "print('DZ = \\n%s' % (np.dot(D, Z),))\n",
    "print('EX = \\n%s' % (np.dot(E, X),))\n",
    "\n",
    "# Visual convergence analysis.\n",
    "print('f_z(Z) + g_z(Z) + f_d(D) + f_e(E) = %f' % (objective_g[-1],))\n",
    "objective = np.concatenate((np.array(objective_z), np.array(objective_d), np.array(objective_e)), axis=1)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective[:, 0], label='Z: data term')\n",
    "plt.semilogy(objective[:, 1], label='Z: prior term')\n",
    "#plt.semilogy(np.sum(objective[:,0:2], axis=1), label='Z: sum')\n",
    "plt.semilogy(objective[:, 2], label='D: data term')\n",
    "plt.semilogy(objective[:, 4], label='E: data term')\n",
    "plt.title('Sub-problems convergence')\n",
    "plt.xlabel('Iteration number (inner loops)')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.legend(); plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.semilogy(objective_g)\n",
    "plt.title('Global convergence')\n",
    "plt.xlabel('Iteration number (outer loop)')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.grid(True); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
