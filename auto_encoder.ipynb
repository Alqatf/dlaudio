{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparse energy auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The definition of the algortihm behind our sparse energy auto-encoder model.\n",
    "* It is an unsupervised feature extraction tool which tries to find a good sparse representation in an efficient manner.\n",
    "* This notebook is meant to be imported by other notebooks for applications to image or audio data.\n",
    "* Modeled after sklearn Estimator class so that it can be integrated into an sklearn Pipeline. Note that matrix dimensions are inverted (code vs math) to follow sklearn conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General problem:  \n",
    "* given $X \\in R^{n \\times N}$,\n",
    "* solve $\\min\\limits_{Z \\in R^{m \\times N}, D \\in R^{n \\times m}, E \\in R^{m \\times n}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2 + \\frac{\\lambda_e}{2} \\|Z - EX\\|_F^2 + \\|Z\\|_1$\n",
    "* s.t. $\\|d_i\\|_2 \\leq 1$, $\\|e_k\\|_2 \\leq 1$, $i = 1, \\ldots, m$, $k = 1, \\ldots, n$\n",
    "\n",
    "which can be reduced to sparse coding with dictionary learning:  \n",
    "* given $X \\in R^{n \\times N}$,\n",
    "* solve $\\min\\limits_{Z \\in R^{m \\times N}, D \\in R^{n \\times m}} \\frac{\\lambda_d}{2} \\|X - DZ\\|_F^2 + \\|Z\\|_1$\n",
    "* s.t. $\\|d_i\\|_2 \\leq 1$, $i = 1, \\ldots, m$\n",
    "\n",
    "Observations:\n",
    "* Almost ten times faster (on comparison_xavier) using optimized linear algebra subroutines:\n",
    "    * None: 9916s\n",
    "    * ATLAS: 1335s (is memory bandwith limited)\n",
    "    * OpenBLAS: 1371s (seems more CPU intensive than ATLAS)\n",
    "\n",
    "Open questions:\n",
    "* First optimize for Z (last impl) or first for D/E (new impl) ?\n",
    "    * Seem to converge much faster if Z optimized last (see comparison_xavier).\n",
    "    * But two times slower.\n",
    "    * In fit we optimize for parameters D, E so it makes sense to optimize them last.\n",
    "* Fast evaluation of la.norm(Z.T.dot(Z)). Cumulative to save memory ?\n",
    "* Consider adding an option for $E = D^T$\n",
    "* Use single precision, i.e. float32 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from pyunlocbox import functions, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def _normalize(X, axis=1):\n",
    "    \"\"\"Normalize the selected axis of an ndarray to unit norm.\"\"\"\n",
    "    return X / np.sqrt(np.sum(X**2, axis))[:,np.newaxis]\n",
    "\n",
    "class auto_encoder():\n",
    "    \"\"\"Sparse energy auto-encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, m=100, ld=None, le=None, lg=None,\n",
    "                 rtol=1e-3, xtol=None, N_inner=100, N_outer=15):\n",
    "        \"\"\"\n",
    "        Model hyper-parameters and solver stopping criteria.\n",
    "        \n",
    "        Model hyper-parameters:\n",
    "            m:  number of atoms in the dictionary, sparse code length\n",
    "            ld: weigth of the dictionary l2 penalty\n",
    "            le: weigth of the encoder l2 penalty\n",
    "            lg: weight of the graph smoothness\n",
    "        \n",
    "        Stopping criteria::\n",
    "            rtol: objective function convergence\n",
    "            xtol: model parameters convergence\n",
    "            N_inner: hard limit of inner iterations\n",
    "            N_outer: hard limit of outer iterations\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.ld = ld\n",
    "        self.le = le\n",
    "        self.lg = lg\n",
    "        self.N_outer = N_outer\n",
    "        \n",
    "        # Solver common parameters.\n",
    "        self.params = {'rtol':       rtol,\n",
    "                       'xtol':       xtol,\n",
    "                       'maxit':      N_inner,\n",
    "                       'verbosity': 'NONE'}\n",
    "\n",
    "    def _convex_functions(self, X, Z):\n",
    "        \"\"\"Define convex functions.\"\"\"\n",
    "        \n",
    "        f = functions.proj_b2()\n",
    "        self.f = functions.func()\n",
    "        self.f._eval = lambda X: 0\n",
    "        self.f._prox = lambda X,_: f._prox(X.T, 1).T\n",
    "        #self.f._prox = lambda X,_: _normalize(X)\n",
    "        \n",
    "        if self.ld is not None:\n",
    "            self.g_d = functions.norm_l2(lambda_=self.ld/2., A=Z, y=X, tight=False)\n",
    "            g_z = functions.norm_l2(lambda_=self.ld/2., A=self.D.T, y=X.T, tight=False)\n",
    "        if self.le is not None:\n",
    "            self.h_e = functions.norm_l2(lambda_=self.le/2., A=X, y=Z, tight=False)\n",
    "            h_z = functions.norm_l2(lambda_=self.le/2., y=lambda: X.dot(self.E).T, tight=True)\n",
    "        \n",
    "        if self.ld is not None and self.le is None:\n",
    "            self.gh_z = g_z\n",
    "        elif self.ld is None and self.le is not None:\n",
    "            self.gh_z = h_z\n",
    "        elif self.ld is not None and self.le is not None:\n",
    "            self.gh_z = functions.func()\n",
    "            self.gh_z._eval = lambda Z: g_z._eval(Z) + h_z._eval(Z)\n",
    "            self.gh_z._grad = lambda Z: g_z._grad(Z) + h_z._grad(Z)\n",
    "        else:\n",
    "            raise ValueError('Either ld or le should be defined.')\n",
    "            \n",
    "        self.i_z = functions.norm_l1()\n",
    "\n",
    "    def _minD(self, X, Z):\n",
    "        \"\"\"Convex minimization for D.\"\"\"\n",
    "        \n",
    "        # Lipschitz continuous gradient. Faster if larger dim is 'inside'.\n",
    "        L = self.ld * la.norm(Z.T.dot(Z))\n",
    "        \n",
    "        solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "        ret = solvers.solve([self.g_d, self.f], self.D, solver, **self.params)\n",
    "        \n",
    "        self.objective_d.extend(ret['objective'])\n",
    "        self.objective_z.extend([[0,0]] * len(ret['objective']))\n",
    "        self.objective_e.extend([[0,0]] * len(ret['objective']))\n",
    "    \n",
    "    def _minE(self, X, Z):\n",
    "        \"\"\"Convex minimization for E.\"\"\"\n",
    "        \n",
    "        # Lipschitz continuous gradient. Faster if larger dim is 'inside'.\n",
    "        L = self.le * la.norm(X.T.dot(X))\n",
    "        \n",
    "        solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "        ret = solvers.solve([self.h_e, self.f], self.E, solver, **self.params)\n",
    "        \n",
    "        self.objective_e.extend(ret['objective'])\n",
    "        self.objective_z.extend([[0,0]] * len(ret['objective']))\n",
    "        self.objective_d.extend([[0,0]] * len(ret['objective']))\n",
    "    \n",
    "    def _minZ(self, X, Z):\n",
    "        \"\"\"Convex minimization for Z.\"\"\"\n",
    "        \n",
    "        L_e = self.le if self.le is not None else 0\n",
    "        L_d = self.ld * la.norm(self.D.T.dot(self.D)) if self.ld is not None else 0\n",
    "        L = L_d + L_e\n",
    "        \n",
    "        solver = solvers.forward_backward(step=1./L, method='FISTA')\n",
    "        ret = solvers.solve([self.gh_z, self.i_z], Z.T, solver, **self.params)\n",
    "        \n",
    "        self.objective_z.extend(ret['objective'])\n",
    "        self.objective_d.extend([[0,0]] * len(ret['objective']))\n",
    "        self.objective_e.extend([[0,0]] * len(ret['objective']))\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model parameters (dictionary, encoder and graph)\n",
    "        given training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (N, n)\n",
    "            Training vectors, where N is the number of samples\n",
    "            and n is the number of features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Z : ndarray, shape (N, m)\n",
    "            Sparse codes (a by-product of training), where N\n",
    "            is the number of samples and m is the number of atoms.\n",
    "        \"\"\"\n",
    "        N, n = X.shape\n",
    "        \n",
    "        # Model parameters initialization.\n",
    "        if self.ld is not None:\n",
    "            self.D = _normalize(np.random.uniform(size=(self.m, n)).astype(X.dtype))\n",
    "        if self.le is not None:\n",
    "            self.E = _normalize(np.random.uniform(size=(n, self.m)).astype(X.dtype))\n",
    "        \n",
    "        # Initial predictions.\n",
    "        #Z = np.random.uniform(size=(N, self.m)).astype(X.dtype)\n",
    "        Z = np.zeros(shape=(N, self.m), dtype=X.dtype)\n",
    "        \n",
    "        # Initialize convex functions.\n",
    "        self._convex_functions(X, Z)\n",
    "        \n",
    "        # Objective functions.\n",
    "        self.objective = []\n",
    "        self.objective_z = []\n",
    "        self.objective_d = []\n",
    "        self.objective_e = []\n",
    "        \n",
    "        # Multi-variate non-convex optimization (outer loop).\n",
    "        for _ in range(self.N_outer):\n",
    "\n",
    "            self._minZ(X, Z)\n",
    "\n",
    "            if self.ld is not None:\n",
    "                self._minD(X, Z)\n",
    "\n",
    "            if self.le is not None:\n",
    "                self._minE(X, Z)\n",
    "\n",
    "            # Global objective function.\n",
    "            self.objective.append(self.gh_z.eval(Z.T) + self.i_z.eval(Z.T))\n",
    "            \n",
    "        return Z\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit to data without returning the transformed data.\"\"\"\n",
    "        self.fit_transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Predict sparse codes for each sample in X.\"\"\"\n",
    "        return self._transform_exact(X)\n",
    "        \n",
    "    def _transform_exact(self, X):\n",
    "        \"\"\"Most accurate but slowest prediction.\"\"\"\n",
    "        N = X.shape[0]\n",
    "        Z = np.random.uniform(size=(N, self.m)).astype(X.dtype)\n",
    "        self._convex_functions(X, Z)\n",
    "        self._minZ(X, Z)\n",
    "        return Z\n",
    "    \n",
    "    def _transform_approx(self, X):\n",
    "        \"\"\"Much faster approximation using only the encoder.\"\"\"\n",
    "        raise NotImplementedError('Not yet implemented')\n",
    "    \n",
    "    def inverse_transform(self, Z):\n",
    "        \"\"\"\n",
    "        Return the data corresponding to the given sparse codes using\n",
    "        the learned dictionary.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not yet implemented')\n",
    "    \n",
    "    def plot_objective(self):\n",
    "        \"\"\"Plot the objective (cost, loss, energy) functions.\"\"\"\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.semilogy(np.array(self.objective_z)[:, 0], label='Z: data term')\n",
    "        plt.semilogy(np.array(self.objective_z)[:, 1], label='Z: prior term')\n",
    "        #plt.semilogy(np.sum(objective[:,0:2], axis=1), label='Z: sum')\n",
    "        plt.semilogy(np.array(self.objective_d)[:, 0], label='D: data term')\n",
    "        plt.semilogy(np.array(self.objective_e)[:, 0], label='E: data term')\n",
    "        niter = np.shape(self.objective_z)[0]\n",
    "        plt.xlim(0, niter-1)\n",
    "        plt.title('Sub-problems convergence')\n",
    "        plt.xlabel('Iteration number (inner loops)')\n",
    "        plt.ylabel('Objective function value')\n",
    "        plt.grid(True); plt.legend(); plt.show()\n",
    "        print('Inner loop: {} iterations'.format(niter))\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.objective)\n",
    "        niter = len(self.objective)\n",
    "        plt.xlim(0, niter-1)\n",
    "        plt.title('Global convergence')\n",
    "        plt.xlabel('Iteration number (outer loop)')\n",
    "        plt.ylabel('Objective function value')\n",
    "        plt.grid(True); plt.show()\n",
    "        print('Outer loop: {} iterations\\n'.format(niter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for solution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools to show model parameters, sparse codes and objective function. The *auto_encoder* class solely contains the core algorithm (and a visualization of the convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(X, Z, D, ld):\n",
    "    \"\"\"Plot the value of the objective function.\"\"\"\n",
    "    g_z = functions.norm_l1()\n",
    "    f_z = functions.norm_l2(lambda_=ld/2., A=D.T, y=X.T, tight=False)\n",
    "    f_d = functions.norm_l2(lambda_=ld/2., A=Z, y=X, tight=False)\n",
    "\n",
    "    g_z = g_z.eval(Z.T)\n",
    "    f_z = f_z.eval(Z.T)\n",
    "    f_d = f_d.eval(D)\n",
    "    assert abs(f_z - f_d) / f_z < 1e-5\n",
    "    \n",
    "    print('||Z||_1 = {:e}'.format(g_z))\n",
    "    print('||X-DZ||_2^2 = {:e}'.format(f_z))\n",
    "    print('||Z||_1 + ||X-DZ||_2^2 = {:e}'.format(g_z + f_z))\n",
    "\n",
    "def sparse_codes(Z, tol=0):\n",
    "    \"\"\"Show the sparsity of the sparse codes.\"\"\"\n",
    "    N, m = Z.shape\n",
    "    \n",
    "    print('Z in [{}, {}]'.format(np.min(Z), np.max(Z)))\n",
    "    \n",
    "    if tol is 0:\n",
    "        nnz = np.count_nonzero(Z)\n",
    "    else:\n",
    "        nnz = np.sum(np.abs(Z) > tol)\n",
    "    print('Sparsity of Z: {:,} non-zero entries out of {:,} entries, '\n",
    "          'i.e. {:.1f}%.'.format(nnz, Z.size, 100.*nnz/Z.size))\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.spy(Z.T, precision=tol, aspect='auto')\n",
    "    plt.xlabel('N = {} samples'.format(N))\n",
    "    plt.ylabel('m = {} atoms'.format(m))\n",
    "    plt.show()\n",
    "    \n",
    "def dictionary(D, tol=1e-7):\n",
    "    \"\"\"Show the norms and sparsity of the learned dictionary.\"\"\"\n",
    "    m, n = D.shape\n",
    "\n",
    "    print('D in [{}, {}]'.format(np.min(D), np.max(D)))\n",
    "    \n",
    "    d = np.sqrt(np.sum(D**2, axis=1))\n",
    "    print('d in [{}, {}]'.format(np.min(d), np.max(d)))\n",
    "    print('Constraints on D: {}'.format(np.alltrue(d <= 1+tol)))\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(d, 'b.')\n",
    "    #plt.ylim(0.5, 1.5)\n",
    "    plt.xlim(0, m-1)\n",
    "    plt.title('Dictionary atom norms')\n",
    "    plt.xlabel('Atom [1,m]')\n",
    "    plt.ylabel('Norm [0,1]')\n",
    "    plt.grid(True); plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.spy(D.T, precision=1e-2, aspect='auto')\n",
    "    plt.xlabel('m = {} atoms'.format(m))\n",
    "    plt.ylabel('data dimensionality of n = {}'.format(n))\n",
    "    plt.show()\n",
    "    \n",
    "    #plt.scatter to show intensity\n",
    "    \n",
    "def atoms(D, Np=None):\n",
    "    \"\"\"\n",
    "    Show dictionary atoms.\n",
    "    \n",
    "    2D atoms if Np is not None, else 1D atoms.\n",
    "    \"\"\"\n",
    "    m, n = D.shape\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    Nx = np.ceil(np.sqrt(m))\n",
    "    Ny = np.ceil(m / float(Nx))\n",
    "    for k in np.arange(m):\n",
    "        ax = fig.add_subplot(Ny, Nx, k)\n",
    "        if Np is not None:\n",
    "            img = D[k,:].reshape(Np, Np)\n",
    "            ax.imshow(img, cmap='gray')  # vmin=0, vmax=1 to disable normalization.\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.plot(D[k,:])\n",
    "            ax.set_xlim(0, n-1)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the auto-encoder class and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # ldd numpy/core/_dotblas.so\n",
    "    try:\n",
    "        import numpy.core._dotblas\n",
    "        print 'fast BLAS'\n",
    "    except ImportError:\n",
    "        print 'slow BLAS'\n",
    "\n",
    "    print np.__version__\n",
    "    np.__config__.show()\n",
    "\n",
    "if False:\n",
    "#if __name__ is '__main__':\n",
    "    import time\n",
    "\n",
    "    # Data.\n",
    "    N, n = 11, 25\n",
    "    X = np.random.normal(size=(N, n))\n",
    "\n",
    "    # Algorithm.\n",
    "    ae = auto_encoder(m=16, le=1, rtol=1e-5, xtol=None).fit(X)\n",
    "    ae = auto_encoder(m=16, ld=1, rtol=1e-5, xtol=None).fit(X)\n",
    "    ae = auto_encoder(m=16, ld=1, le=1, rtol=None, xtol=1e-5)\n",
    "    tstart = time.time()\n",
    "    Z = ae.fit_transform(X)\n",
    "    print('Elapsed time: {:.3f} seconds'.format(time.time() - tstart))\n",
    "    ae.plot_objective()\n",
    "    \n",
    "    assert la.norm(Z - ae.transform(X)) / np.sqrt(Z.size) < 1e-3\n",
    "\n",
    "    # Results visualization.\n",
    "    objective(X, Z, ae.D, 1)\n",
    "    sparse_codes(Z)\n",
    "    dictionary(ae.D)\n",
    "    atoms(ae.D, 5)  # 2D atoms.\n",
    "    atoms(ae.D)  # 1D atoms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
