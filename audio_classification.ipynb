{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy auto-encoder: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technically akin to \"transductive learning\" because the sparse auto-encoder dictionary is learned over the whole dataset, not only the training data. The LeCun paper we compare with does the same. This could be solved by including the dictionary learning step in the classifier. Technical solutions:\n",
    "    1. Compute the dictionary in our custom classifier.\n",
    "    2. Create a scikit-learn Pipeline which includes the whole preprocessing and feature extraction steps.\n",
    "    In either case the ability to import functions from other notebooks would help. This would be very slow due to the tremendous amount of time needed to train the auto-encoder.\n",
    "* We should use \"grid search\" to find the optimal hyper-parameters (auto-encoders, frames, feature vectors, SVM).\n",
    "* We may use a validation set to mitigate the leak of the testing set in the model as we tune the hyper-parameters.\n",
    "* Even if not stated in LeCun's paper we should rescale the data before SVM classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Software versions:')\n",
    "for pkg in [np, sklearn]:\n",
    "    print('  {}: {}'.format(pkg.__name__, pkg.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Retrieve data from the HDF5 data store.\n",
    "2. Choose the data we want to work with:\n",
    "    * raw audio $X_a$,\n",
    "    * CQT spectrograms $X_s$,\n",
    "    * normalized spectrograms $X_n$,\n",
    "    * sparse codes $Z$.\n",
    "3. Eventually reduce the number $N_{genres} \\cdot N_{clips}$ of samples for quicker analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'audio_v2_full.hdf5')\n",
    "audio = h5py.File(filename, 'r')\n",
    "\n",
    "# Display HDF5 attributes.\n",
    "print('Attributes:')\n",
    "for attr in audio.attrs:\n",
    "    print('  {} = {}'.format(attr, audio.attrs[attr]))\n",
    "\n",
    "# Show datasets, their dimensionality and data type.\n",
    "print('Datasets:')\n",
    "for dname, dset in audio.items():\n",
    "    print('  {:2}: {:24}, {}'.format(dname, dset.shape, dset.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datinfo(X, name='Dataset'):\n",
    "    r\"\"\"Print dataset size and dimensionality\"\"\"\n",
    "    print('{}:\\n'\n",
    "          '  size: N={:,} x n={} -> {:,} floats\\n'\n",
    "          '  dim: {:,} features per clip\\n'\n",
    "          '  shape: {}'\n",
    "          .format(name, np.prod(X.shape[:-1]), X.shape[-1],\n",
    "                  np.prod(X.shape), np.prod(X.shape[2:]), X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose dataset.\n",
    "X = audio.get('Xs')\n",
    "\n",
    "# Full dataset.\n",
    "Ngenres, Nclips, Nframes, _, n = audio[dname].shape\n",
    "datinfo(X, 'Full dataset')\n",
    "print(type(X))\n",
    "\n",
    "# Reduce data size.\n",
    "#Ngenres, Nclips = 4, 100\n",
    "\n",
    "# Load data into memory as a standard NumPy array.\n",
    "X = X[:Ngenres,:Nclips,:,:,:]\n",
    "datinfo(X, 'Reduced dataset')\n",
    "print(type(X))\n",
    "\n",
    "# Resize in place without memory loading via hyperslab.\n",
    "# Require chunked datasets.\n",
    "#X.resize((Ngenres, Nclips, Nframes, 2, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vectors through aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another (hopefully intelligent) dimensionality reduction:\n",
    "* Aggregation of features from various frames to make up $2N_{vectors} = 12$ feature vectors per clip. Each vector represents approximatly 5 seconds of audio which is way longer than single frames while shorter than the whole clip.\n",
    "* There is again a 50% overlap between those feature vectors.\n",
    "* Absolute value rectification to prevent components of different sign from canceling each other out.\n",
    "* Can be thought as an histogram of used dictionary atoms (if using $Z$) or frequency bins (if using $X_s$) along the chosen time window.\n",
    "* Note that feature aggregation does not make much sense for raw audio (if using $X_a$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten consecutive frames in time.\n",
    "X1 = X.reshape((Ngenres, Nclips, 2*Nframes, n), order='C')\n",
    "assert np.all(X1[1,4,3,:] == X[1,4,1,1,:])\n",
    "datinfo(X1, 'Flattened frames')\n",
    "\n",
    "# Parameters.\n",
    "Nvectors = 6\n",
    "Nframes_per_vector = int(np.floor(2 * Nframes / (Nvectors+0.5)))\n",
    "\n",
    "def aggregate(X):\n",
    "    # Truncate.\n",
    "    X = X[:,:,:Nvectors*Nframes_per_vector,:]\n",
    "    # Group.\n",
    "    X = X.reshape((Ngenres, Nclips, Nvectors, Nframes_per_vector, n))\n",
    "    datinfo(X, 'Truncated and grouped')\n",
    "    # Aggregate.\n",
    "    return np.sum(np.abs(X), axis=3)\n",
    "\n",
    "# Feature vectors.\n",
    "Y = np.empty((Ngenres, Nclips, Nvectors, 2, n))\n",
    "Y[:,:,:,0,:] = aggregate(X1)  # Aligned.\n",
    "Y[:,:,:,1,:] = aggregate(X1[:,:,Nframes_per_vector/2:,:])  # Ovelapped.\n",
    "datinfo(Y, 'Feature vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vectors visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all feature vectors of a given clip.\n",
    "\n",
    "Observations:\n",
    "* Classical music seems to have a much denser spectrum than blues, which may explain why these two classes are easily identifiable using $X_s$.\n",
    "* Country seems to have strong low frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre, clip = 0, 7\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "fig.suptitle('12 feature vectors each covering 5 seconds with 50% overlap')\n",
    "for vector in range(Nvectors):\n",
    "    for k in range(2):\n",
    "        i = vector*2+k\n",
    "        ax = fig.add_subplot(4, 3, i)\n",
    "        ax.plot(Y[genre,clip,vector,k,:])\n",
    "        ax.set_xlim((0, n))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rearrange dataset as a 2D array: number of samples x dimensionality.\n",
    "2. Optionally scale the data.\n",
    "3. Generate labels.\n",
    "4. Optionally split in training and testing sets.\n",
    "5. Optionally randomize labels for testing.\n",
    "\n",
    "Observations:\n",
    "* Scaling is necessary for classification performance (both accuracy and speed). 'std' scaling is not well suited to our histogram-like feature vectors which are not at all Gaussian distributions. Prefer 'minmax', i.e. scale features in [0,1]. Moreover this scaling will preserve the sparsity when dealing with sparse codes $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepdata(a, b, c, test_size=None, scale=None, rand=False):\n",
    "    \"\"\"Prepare data for classification.\"\"\"\n",
    "    \n",
    "    # Squeeze dataset to a 2D array.\n",
    "    data = Y.reshape((a*b), c)\n",
    "    if c == n:\n",
    "        assert np.all(data[31,:] == Y[0,2,3,1,:])\n",
    "    elif c == Nvectors*2*n:\n",
    "        assert np.all(data[Nclips+2,:] == Y[1,2,:,:,:].reshape(-1))\n",
    "\n",
    "    # Independently scale each feature.\n",
    "    # Put in an sklearn Pipeline to avoid transductive learning.\n",
    "    if scale is 'std':\n",
    "        # Features have zero norm and unit standard deviation.\n",
    "        data = preprocessing.scale(data, axis=0)\n",
    "    elif scale is 'minmax':\n",
    "        # Features in [0,1].\n",
    "        data = data - np.min(data, axis=0)\n",
    "        data = data / np.max(data, axis=0)\n",
    "    #print(np.min(data, axis=0))\n",
    "    #print(np.max(data, axis=0))\n",
    "    \n",
    "    # Labels.\n",
    "    target = np.empty((a, b), dtype=np.uint8)\n",
    "    for genre in range(Ngenres):\n",
    "        target[genre,:] = genre\n",
    "    target.resize(data.shape[0])\n",
    "    print('{} genres: {}'.format(Ngenres, ', '.join(audio.attrs['labels'][:Ngenres])))\n",
    "\n",
    "    # Be sure that classification with random labels is no better than random.\n",
    "    if rand:\n",
    "        target = np.floor(np.random.uniform(0, Ngenres, target.shape))\n",
    "        print('Balance: {} {}'.format(np.sum(target == 0), np.sum(target == 1)))\n",
    "\n",
    "    # Training and testing sets.\n",
    "    if test_size is not None:\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "            data, target, test_size=test_size)  # random_state=1\n",
    "        print('Training data: {}, {}'.format(X_train.shape, X_train.dtype))\n",
    "        print('Testing data: {}, {}'.format(X_test.shape, X_test.dtype))\n",
    "        print('Training labels: {}, {}'.format(y_train.shape, y_train.dtype))\n",
    "        print('Testing labels: {}, {}'.format(y_test.shape, y_test.dtype))\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        print('Data: {}, {}'.format(data.shape, data.dtype))\n",
    "        print('Labels: {}, {}'.format(target.shape, target.dtype))\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each feature vector gets a genre label.\n",
    "* Classification with linear Vector Support Machine (SVM).\n",
    "* Fast to train.\n",
    "* Scale well to large dataset.\n",
    "* Two implementations: liblinear (sklearn LinearSVC) and libsvm (sklearn SVC and NuSVC)\n",
    "* Multi-class: \"one-vs-one\" approach (Knerr et al., 1990) (sklearn SVC and NuSVC) and \"one-vs-the-rest\" (sklearn LinearSVC)\n",
    "\n",
    "Observations:\n",
    "* We can predict genre labels of individual frames with good accuracy using CQT spectrograms only.\n",
    "* SVC vs NuSVC vs LinearSVC:\n",
    "    * 10-fold cross-validation with 10 classes (default $C=1$ and $\\nu=0.5$):\n",
    "        * SVC (0.56) yields better accuracy than LinearSVC (0.53) than NuSVC (0.51)\n",
    "        * SVC (303s) and LinearSVC (296s) faster than NuSVC (501s)\n",
    "    * SVC does often not converge if data is not scaled\n",
    "    * LinearSVC may be more scalable (in the number of samples)\n",
    "* Hyper-parameters:\n",
    "    * $C$ seems to have little impact.\n",
    "    * $\\nu$ has a great impact on speed: lower is slower\n",
    "\n",
    "Open questions:\n",
    "* Which multi-class strategy to adopt: one-vs-all or one-vs-one ?\n",
    "    * sklearn states that one-vs-all is the most common strategy\n",
    "* Determine $C$ or $\\nu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a classifier.\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear', C=1)\n",
    "#clf_svm = svm.NuSVC(kernel='linear', nu=0.5)\n",
    "#clf_svm = svm.LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the single feature vector classifier (linear SVM).\n",
    "if True:\n",
    "    \n",
    "    # Split data.\n",
    "    X_train, X_test, y_train, y_test = prepdata(\n",
    "        Ngenres, Nclips*Nvectors*2, n, test_size=0.4,\n",
    "        scale='minmax', rand=False)\n",
    "    \n",
    "    # Train.\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Test.\n",
    "    y_predict = clf_svm.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_predict)\n",
    "    print('Accuracy: {:.1f} %'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dimensionality reduction step:\n",
    "* Each of the 12 feature vectors of a clip gives a vote. We choose the genre with the highest number of votes.\n",
    "* Implemented as a custom classifier which embeds an SVM for individual feature vectors classification.\n",
    "* Alternative implementation: insert in a sklearn pipeline after SVC.\n",
    "\n",
    "Observations:\n",
    "* Accuracy on whole clips is indeed better than accuracy on individual feature vectors.\n",
    "* *clf_svm_vote.confidence* is useful to observe if a class is harder to differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and instantiate our custom classifier.\n",
    "class svm_vote(sklearn.base.BaseEstimator):\n",
    "    \n",
    "    def __init__(self, svm):\n",
    "        self.svm = svm\n",
    "    \n",
    "    def _vectors(self, X, y=None):\n",
    "        \"\"\"Rearrange data in feature vectors for SVM.\"\"\"\n",
    "        X = X.reshape(X.shape[0]*Nvectors*2, n)\n",
    "        if y is not None:\n",
    "            y = np.repeat(y, Nvectors*2, axis=0)\n",
    "            assert y.shape[0] == X.shape[0]\n",
    "            return (X, y)\n",
    "        else:\n",
    "            return (X,)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the embedded SVC.\"\"\"\n",
    "        self.svm.fit(*self._vectors(X, y))\n",
    "    \n",
    "    def svm_score(self, X, y):\n",
    "        \"\"\"Return SVC accuracy on feature vectors.\"\"\"\n",
    "        return self.svm.score(*self._vectors(X, y))\n",
    "    \n",
    "    def svm_predict(self, X):\n",
    "        \"\"\"Return SVC predictions on feature vectors.\"\"\"\n",
    "        y = self.svm.predict(*self._vectors(X))\n",
    "        y.resize(X.shape[0], Nvectors*2)\n",
    "        return y\n",
    "        \n",
    "    def confidence(self, X):\n",
    "        \"\"\"Return the number of votes for each class.\"\"\"\n",
    "        def bincount(x):\n",
    "            return np.bincount(x, minlength=Ngenres)\n",
    "        y = np.apply_along_axis(bincount, 1, self.svm_predict(X))\n",
    "        assert np.all(np.sum(y, axis=1) == Nvectors*2)\n",
    "        return y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predictions on whole clips.\"\"\"\n",
    "        y = self.svm_predict(X)\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), 1, y)\n",
    "        #return np.zeros(X.shape[0])  # Pretty bad prediction.\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Return the accuracy score. Used by sklearn cross-validation.\"\"\"\n",
    "        return metrics.accuracy_score(y, self.predict(X))\n",
    "\n",
    "clf_svm_vote = svm_vote(clf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the whole clip classifier (linear SVM and majority voting).\n",
    "if True:\n",
    "    \n",
    "    # Split data.\n",
    "    X_train, X_test, y_train, y_test = prepdata(\n",
    "        Ngenres, Nclips, Nvectors*2*n, test_size=0.4,\n",
    "        scale='minmax', rand=False)\n",
    "    \n",
    "    # Train.\n",
    "    clf_svm_vote.fit(X_train, y_train)\n",
    "    \n",
    "    # Test on single vectors.\n",
    "    acc = clf_svm_vote.svm_score(X_test, y_test)\n",
    "    print('Feature vectors accuracy: {:.1f} %'.format(acc*100))\n",
    "    \n",
    "    # Observe individual votes.\n",
    "    #print(clf_svm_vote.svm_predict(X_test))\n",
    "    #print(clf_svm_vote.confidence(X_test))\n",
    "    \n",
    "    # Test on whole clips.\n",
    "    y_predict = clf_svm_vote.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_predict)\n",
    "    assert acc == clf_svm_vote.score(X_test, y_test)\n",
    "    print('Clips accuracy: {:.1f} %'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 10-fold cross-validation.\n",
    "* 100 randomly chosen clips per fold.\n",
    "* 9 folds (900 clips) for training, 1 fold (100 clips) for testing.\n",
    "* Determine a classification accuracy using testing set.\n",
    "* Repeat 10 times: mean and standard deviation.\n",
    "\n",
    "Observations:\n",
    "* Data should be shuffled as samples with the same label are contiguous, i.e. data ordering is not arbitrary.\n",
    "* *ShuffleSplit*, *StratifiedShuffleSplit*, *KFold* and *StratifiedKFold* yields similar results as long as data is shuffeld.\n",
    "* (Lots of variance between runs.)\n",
    "* Data should be rescaled for good performance (both accuracy and speed).\n",
    "\n",
    "Results:\n",
    "* With $X_s$\n",
    "    * Accuracy of 95 (+/- 5) for 2 genres (SVC, minmax)\n",
    "    * Accuracy of 81 (+/- 4) for 4 genres (SVC, minmax)\n",
    "    * Accuracy of 56 (+/- 5) for 10 genres (SVC, minmax)\n",
    "\n",
    "Ideas:\n",
    "* Use the area under the receiver operating characteristing (ROC) curve (AUC). Not sure if applicable to multi-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, target = prepdata(Ngenres, Nclips, Nvectors*2*n, scale='minmax')\n",
    "\n",
    "# Cross validation iterators.\n",
    "#cv = cross_validation.ShuffleSplit(Ngenres*Nclips, n_iter=10, test_size=0.1)\n",
    "#cv = cross_validation.StratifiedShuffleSplit(target, test_size=0.4)\n",
    "cv = cross_validation.KFold(Ngenres*Nclips, shuffle=True, n_folds=10)\n",
    "#cv = cross_validation.StratifiedKFold(target, shuffle=True, n_folds=10)\n",
    "\n",
    "tstart = time.time()\n",
    "scores = cross_validation.cross_val_score(\n",
    "    clf_svm_vote, data, target, cv=cv, n_jobs=1)\n",
    "print('Elapsed time: {:.2f} seconds'.format(time.time() - tstart))\n",
    "\n",
    "print('Scores:\\n{}'.format(scores))\n",
    "print('Accuracy: {:.0f} (+/- {:.1f})'.format(scores.mean()*100, scores.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
